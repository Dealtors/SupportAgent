import joblib
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Any, Optional
import warnings
from dataclasses import dataclass
from pathlib import Path
import json
from datetime import datetime

# ML –∏–º–ø–æ—Ä—Ç—ã
from sentence_transformers import SentenceTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from scipy.stats import uniform
from sklearn.metrics import classification_report, accuracy_score, f1_score
from sklearn.base import BaseEstimator, TransformerMixin

# –î–ª—è —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏
import torch
from transformers import pipeline

warnings.filterwarnings('ignore')

@dataclass
class Document:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
    text: str
    source: str
    embedding: Optional[np.ndarray] = None
    toxicity_score: Optional[float] = None
    metadata: Optional[Dict] = None

class ToxicityChecker:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
    
    def __init__(self, model_name="sismetanin/rubert-toxic-detection"):
        try:
            self.toxicity_classifier = pipeline(
                "text-classification",
                model=model_name,
                tokenizer=model_name,
                device=0 if torch.cuda.is_available() else -1
            )
            print("‚úÖ –ú–æ–¥–µ–ª—å —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏: {e}")
            self.toxicity_classifier = None
    
    def check_toxicity(self, text: str) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞"""
        if not self.toxicity_classifier:
            return {"error": "–ú–æ–¥–µ–ª—å —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"}
        
        try:
            result = self.toxicity_classifier(text[:512])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            toxicity_score = result[0]['score'] if result[0]['label'] == 'toxic' else 1 - result[0]['score']
            
            return {
                'is_toxic': toxicity_score > 0.7,
                'toxicity_score': float(toxicity_score),
                'label': result[0]['label'],
                'confidence': float(result[0]['score'])
            }
        except Exception as e:
            return {"error": f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏: {e}"}

class EmbeddingStorage:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""
    
    def __init__(self, embedding_model_name='cointegrated/rubert-tiny2'):
        self.embedding_model = SentenceTransformer(embedding_model_name)
        self.documents: List[Document] = []
        self.embeddings_matrix: Optional[np.ndarray] = None
    
    def add_document(self, text: str, source: str, metadata: Dict = None) -> str:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –µ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥"""
        doc_id = f"doc_{len(self.documents)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
        embedding = self.embedding_model.encode(text)
        
        document = Document(
            text=text,
            source=source,
            embedding=embedding,
            metadata=metadata or {}
        )
        
        self.documents.append(document)
        self._update_embeddings_matrix()
        
        return doc_id
    
    def _update_embeddings_matrix(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –º–∞—Ç—Ä–∏—Ü—É —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        if self.documents:
            self.embeddings_matrix = np.array([doc.embedding for doc in self.documents])
    
    def search_similar(self, query: str, top_k: int = 5) -> List[Tuple[Document, float]]:
        """–ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        if not self.documents:
            return []
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.embedding_model.encode(query)
        
        # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarities = np.dot(self.embeddings_matrix, query_embedding) / (
            np.linalg.norm(self.embeddings_matrix, axis=1) * np.linalg.norm(query_embedding)
        )
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0:  # –¢–æ–ª—å–∫–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                results.append((self.documents[idx], float(similarities[idx])))
        
        return results
    
    def save_embeddings(self, filepath: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ —Ñ–∞–π–ª"""
        data = {
            'documents': [
                {
                    'text': doc.text,
                    'source': doc.source,
                    'embedding': doc.embedding.tolist() if doc.embedding is not None else None,
                    'metadata': doc.metadata
                }
                for doc in self.documents
            ]
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def load_embeddings(self, filepath: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.documents = []
        for doc_data in data['documents']:
            document = Document(
                text=doc_data['text'],
                source=doc_data['source'],
                embedding=np.array(doc_data['embedding']) if doc_data['embedding'] else None,
                metadata=doc_data.get('metadata', {})
            )
            self.documents.append(document)
        
        self._update_embeddings_matrix()

class SentenceTransformerEmbedder(BaseEstimator, TransformerMixin):
    """–í—Ä–∞–ø–ø–µ—Ä –¥–ª—è SentenceTransformer –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ sklearn pipeline"""
    
    def __init__(self, model_name='cointegrated/rubert-tiny2'):
        self.model_name = model_name
        self.model = None
        
    def fit(self, X, y=None):
        self.model = SentenceTransformer(self.model_name)
        return self
        
    def transform(self, X):
        return self.model.encode(X)
    
    def fit_transform(self, X, y=None):
        self.fit(X, y)
        return self.transform(X)

class EnhancedClassifierAgent:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å—é –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""
    
    def __init__(self, model_path: str = None):
        self.model = None
        self.toxicity_checker = ToxicityChecker()
        self.embedding_storage = EmbeddingStorage()
        
        # –î–µ–º–æ-–¥–∞–Ω–Ω—ã–µ (–≤–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥)
        self.demo_texts = [...]  # –í–∞—à–∏ demo_texts
        self.demo_labels = [...]  # –í–∞—à–∏ demo_labels
        self.class_descriptions = {...}  # –í–∞—à–∏ class_descriptions
        
        if model_path:
            self.load_model(model_path)
        else:
            self._create_demo_model()
    
    def _create_demo_model(self):
        """–°–æ–∑–¥–∞–µ—Ç –¥–µ–º–æ-–º–æ–¥–µ–ª—å"""
        from sklearn.pipeline import Pipeline
        from sklearn.linear_model import LogisticRegression
        
        self.model = Pipeline([
            ('embedder', SentenceTransformerEmbedder()),
            ('clf', LogisticRegression(random_state=42, max_iter=1000))
        ])
        
        self.model.fit(self.demo_texts, self.demo_labels)
        print("‚úÖ –î–µ–º–æ-–º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")
    
    def predict_with_toxicity(self, text: str) -> Dict[str, Any]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å
        toxicity_result = self.toxicity_checker.check_toxicity(text)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        if self.model:
            try:
                category, confidence = self.predict(text)
                category_result = {
                    'category': category,
                    'confidence': confidence,
                    'description': self.class_descriptions.get(category, '')
                }
            except Exception as e:
                category_result = {'error': f'–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}'}
        else:
            category_result = {'error': '–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞'}
        
        return {
            'text': text,
            'toxicity': toxicity_result,
            'classification': category_result
        }
    
    def predict(self, text: str) -> Tuple[str, float]:
        """–ë–∞–∑–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        if not self.model:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        probabilities = self.model.predict_proba([text])[0]
        max_prob_idx = np.argmax(probabilities)
        confidence = probabilities[max_prob_idx]
        predicted_class = self.model.classes_[max_prob_idx]
        
        return predicted_class, float(confidence)
    
    def add_to_embedding_storage(self, text: str, source: str, metadata: Dict = None):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        return self.embedding_storage.add_document(text, source, metadata)
    
    def search_similar_texts(self, query: str, top_k: int = 5):
        """–ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ —Ç–µ–∫—Å—Ç—ã"""
        return self.embedding_storage.search_similar(query, top_k)
    
    def save_model(self, model_path: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å"""
        if self.model:
            joblib.dump(self.model, model_path)
            print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
    
    def load_model(self, model_path: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å"""
        try:
            self.model = joblib.load(model_path)
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_path}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            self._create_demo_model()
    
    def save_embeddings(self, filepath: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
        self.embedding_storage.save_embeddings(filepath)
        print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filepath}")
    
    def load_embeddings(self, filepath: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
        self.embedding_storage.load_embeddings(filepath)
        print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ {filepath}")

# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã
if __name__ == "__main__":
    agent = EnhancedClassifierAgent()
    
    # –¢–µ—Å—Ç —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    test_texts = [
        "–Ω–µ –º–æ–≥—É –≤–æ–π—Ç–∏ –≤ –∞–∫–∫–∞—É–Ω—Ç, —ç—Ç–∞ –¥—É—Ä–∞—Ü–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–ø—è—Ç—å –≥–ª—é—á–∏—Ç",
        "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≤—ã–ª–µ—Ç–∞–µ—Ç –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ, –ø–æ–º–æ–≥–∏—Ç–µ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞",
        "–≤—ã –≤—Å–µ –∏–¥–∏–æ—Ç—ã, –Ω–∏—á–µ–≥–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç!",
        "–∫–∞–∫ –ø–æ–º–µ–Ω—è—Ç—å email –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –ø—Ä–æ—Ñ–∏–ª—è?"
    ]
    
    for text in test_texts:
        result = agent.predict_with_toxicity(text)
        print(f"\nüìù –¢–µ–∫—Å—Ç: '{text}'")
        print(f"üîû –¢–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å: {result['toxicity']}")
        print(f"üéØ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {result['classification']}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        if 'error' not in result['classification']:
            agent.add_to_embedding_storage(text, "test_input")
    
    # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
    print("\nüîç –ü–û–ò–°–ö –ü–û–•–û–ñ–ò–• –¢–ï–ö–°–¢–û–í:")
    similar = agent.search_similar_texts("–ø—Ä–æ–±–ª–µ–º–∞ —Å –≤—Ö–æ–¥–æ–º –≤ —Å–∏—Å—Ç–µ–º—É")
    for doc, score in similar:
        print(f"  –°—Ö–æ–¥—Å—Ç–≤–æ: {score:.3f} - '{doc.text[:50]}...'")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    agent.save_embeddings("text_embeddings.json")