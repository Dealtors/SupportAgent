from enhanced_classifier import EnhancedClassifierAgent
from document_parser import DocumentParser
import json
from typing import Dict
from pathlib import Path

class DocumentProcessingSystem:
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        self.classifier = EnhancedClassifierAgent()
        self.parser = DocumentParser()
        self.processed_documents = []
    
    def process_document(self, file_path: str, use_ocr: bool = True):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç: –ø–∞—Ä—Å–∏—Ç, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å, –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç"""
        print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É: {file_path}")
        
        # –ü–∞—Ä—Å–∏–º –¥–æ–∫—É–º–µ–Ω—Ç
        parsed_pages = self.parser.parse_document(file_path, use_ocr)
        
        results = []
        for page in parsed_pages:
            if page['text'].strip():  # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                page_result = self._analyze_page(page, file_path)
                results.append(page_result)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                if 'classification' in page_result and 'error' not in page_result['classification']:
                    self.classifier.add_to_embedding_storage(
                        page['text'],
                        f"{file_path}_page_{page['page_number']}",
                        {
                            'file_path': file_path,
                            'page_number': page['page_number'],
                            'parsing_method': page['method']
                        }
                    )
        
        self.processed_documents.append({
            'file_path': file_path,
            'pages': results
        })
        
        return results
    
    def _analyze_page(self, page: Dict, file_path: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        text = page['text']
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        analysis_result = self.classifier.predict_with_toxicity(text)
        
        return {
            'page_number': page['page_number'],
            'parsing_method': page['method'],
            'text_preview': text[:200] + "..." if len(text) > 200 else text,
            'analysis': analysis_result
        }
    
    def search_in_documents(self, query: str, top_k: int = 5):
        """–ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ —Ç–µ–∫—Å—Ç—ã –≤–æ –≤—Å–µ—Ö –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö"""
        return self.classifier.search_similar_texts(query, top_k)
    
    def save_results(self, output_file: str = "processing_results.json"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        data = {
            'processed_documents': self.processed_documents,
            'timestamp': str(Path(output_file).stat().st_mtime if Path(output_file).exists() else None)
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        self.classifier.save_embeddings("document_embeddings.json")
        
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤—Å–µ–π —Å–∏—Å—Ç–µ–º—ã
if __name__ == "__main__":
    system = DocumentProcessingSystem()
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
    results = system.process_document("example.pdf", use_ocr=True)
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    for result in results:
        print(f"\nüìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {result['page_number']}:")
        print(f"–ú–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞: {result['parsing_method']}")
        print(f"–¢–µ–∫—Å—Ç: {result['text_preview']}")
        
        analysis = result['analysis']
        print(f"üîû –¢–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å: {analysis['toxicity']}")
        print(f"üéØ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {analysis['classification']}")
    
    # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ —Ç–µ–∫—Å—Ç—ã
    print("\nüîç –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤:")
    similar = system.search_in_documents("—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞")
    for doc, score in similar:
        print(f"  –°—Ö–æ–¥—Å—Ç–≤–æ: {score:.3f}")
        print(f"  –ò—Å—Ç–æ—á–Ω–∏–∫: {doc.source}")
        print(f"  –¢–µ–∫—Å—Ç: {doc.text[:100]}...")
        print()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    system.save_results()